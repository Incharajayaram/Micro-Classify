{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, base_dir, subfolders, transform=None):\n",
    "        self.base_dir = base_dir\n",
    "        self.subfolders = subfolders\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for subfolder in subfolders:\n",
    "            folder_path = os.path.join(base_dir, subfolder)\n",
    "            label = subfolder\n",
    "\n",
    "            for img_name in os.listdir(folder_path):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(folder_path, img_name)\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "        \n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "# Define base directory and subfolders for different classes\n",
    "base_dir = r'DIAT-uSAT_dataset'\n",
    "subfolders = [\n",
    "    r\"3_long_blade_rotor\", \n",
    "    r\"3_short_blade_rotor\", \n",
    "    r\"Bird\", \n",
    "    r\"Bird+mini-helicopter\", \n",
    "    r\"drone\", \n",
    "    r\"rc_plane\", \n",
    "]\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 3 * 224 * 224  # 3 channels, image size 224x224\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = len(subfolders)\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "best_val_accuracy = 0\n",
    "best_model_path = 'best_model_CustomVGGWithAttentionattention.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and dataloaders\n",
    "dataset = CustomImageDataset(base_dir, subfolders, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMImageClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMImageClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer (input_size will be the flattened size of image)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMImageClassifier(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.7197\n",
      "Validation Accuracy after Epoch 1: 29.59%\n",
      "Epoch [2/50], Loss: 1.5413\n",
      "Validation Accuracy after Epoch 2: 33.81%\n",
      "Epoch [3/50], Loss: 1.5228\n",
      "Validation Accuracy after Epoch 3: 34.74%\n",
      "Epoch [4/50], Loss: 1.4690\n",
      "Validation Accuracy after Epoch 4: 31.86%\n",
      "Epoch [5/50], Loss: 1.4741\n",
      "Validation Accuracy after Epoch 5: 31.55%\n",
      "Epoch [6/50], Loss: 1.4963\n",
      "Validation Accuracy after Epoch 6: 28.97%\n",
      "Epoch [7/50], Loss: 1.5950\n",
      "Validation Accuracy after Epoch 7: 25.15%\n",
      "Epoch [8/50], Loss: 1.5778\n",
      "Validation Accuracy after Epoch 8: 30.82%\n",
      "Epoch [9/50], Loss: 1.5524\n",
      "Validation Accuracy after Epoch 9: 31.44%\n",
      "Epoch [10/50], Loss: 1.5683\n",
      "Validation Accuracy after Epoch 10: 30.52%\n",
      "Epoch [11/50], Loss: 1.5321\n",
      "Validation Accuracy after Epoch 11: 26.91%\n",
      "Epoch [12/50], Loss: 1.5207\n",
      "Validation Accuracy after Epoch 12: 30.62%\n",
      "Epoch [13/50], Loss: 1.5247\n",
      "Validation Accuracy after Epoch 13: 31.96%\n",
      "Epoch [14/50], Loss: 1.4937\n",
      "Validation Accuracy after Epoch 14: 33.40%\n",
      "Epoch [15/50], Loss: 1.5044\n",
      "Validation Accuracy after Epoch 15: 28.04%\n",
      "Epoch [16/50], Loss: 1.4856\n",
      "Validation Accuracy after Epoch 16: 31.03%\n",
      "Epoch [17/50], Loss: 1.5082\n",
      "Validation Accuracy after Epoch 17: 31.96%\n",
      "Epoch [18/50], Loss: 1.5082\n",
      "Validation Accuracy after Epoch 18: 25.46%\n",
      "Epoch [19/50], Loss: 1.5761\n",
      "Validation Accuracy after Epoch 19: 28.35%\n",
      "Epoch [20/50], Loss: 1.5576\n",
      "Validation Accuracy after Epoch 20: 31.24%\n",
      "Epoch [21/50], Loss: 1.5260\n",
      "Validation Accuracy after Epoch 21: 30.00%\n",
      "Epoch [22/50], Loss: 1.5129\n",
      "Validation Accuracy after Epoch 22: 27.53%\n",
      "Epoch [23/50], Loss: 1.5079\n",
      "Validation Accuracy after Epoch 23: 31.55%\n",
      "Epoch [24/50], Loss: 1.4771\n",
      "Validation Accuracy after Epoch 24: 33.40%\n",
      "Epoch [25/50], Loss: 1.4519\n",
      "Validation Accuracy after Epoch 25: 31.34%\n",
      "Epoch [26/50], Loss: 1.4534\n",
      "Validation Accuracy after Epoch 26: 33.20%\n",
      "Epoch [27/50], Loss: 1.4594\n",
      "Validation Accuracy after Epoch 27: 31.13%\n",
      "Epoch [28/50], Loss: 1.4677\n",
      "Validation Accuracy after Epoch 28: 26.91%\n",
      "Epoch [29/50], Loss: 1.4647\n",
      "Validation Accuracy after Epoch 29: 32.68%\n",
      "Epoch [30/50], Loss: 1.4663\n",
      "Validation Accuracy after Epoch 30: 31.34%\n",
      "Epoch [31/50], Loss: 1.4574\n",
      "Validation Accuracy after Epoch 31: 33.40%\n",
      "Epoch [32/50], Loss: 1.4533\n",
      "Validation Accuracy after Epoch 32: 28.14%\n",
      "Epoch [33/50], Loss: 1.4444\n",
      "Validation Accuracy after Epoch 33: 33.40%\n",
      "Epoch [34/50], Loss: 1.4457\n",
      "Validation Accuracy after Epoch 34: 33.30%\n",
      "Epoch [35/50], Loss: 1.4479\n",
      "Validation Accuracy after Epoch 35: 33.51%\n",
      "Epoch [36/50], Loss: 1.4529\n",
      "Validation Accuracy after Epoch 36: 31.55%\n",
      "Epoch [37/50], Loss: 1.5729\n",
      "Validation Accuracy after Epoch 37: 24.12%\n",
      "Epoch [38/50], Loss: 1.6040\n",
      "Validation Accuracy after Epoch 38: 28.25%\n",
      "Epoch [39/50], Loss: 1.6010\n",
      "Validation Accuracy after Epoch 39: 24.23%\n",
      "Epoch [40/50], Loss: 1.5084\n",
      "Validation Accuracy after Epoch 40: 33.20%\n",
      "Epoch [41/50], Loss: 1.4540\n",
      "Validation Accuracy after Epoch 41: 31.24%\n",
      "Epoch [42/50], Loss: 1.4488\n",
      "Validation Accuracy after Epoch 42: 30.21%\n",
      "Epoch [43/50], Loss: 1.5049\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        # Flatten the image from (batch_size, channels, height, width) to (batch_size, 1, input_size)\n",
    "        imgs = imgs.view(imgs.size(0), 1, -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            imgs = imgs.view(imgs.size(0), 1, -1)  # Flatten for validation\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f'Validation Accuracy after Epoch {epoch+1}: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Save the best model if validation accuracy improves\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model\n",
    "model = LSTMImageClassifier(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()  # Set model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader, model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            \n",
    "            # Flatten the image from (batch_size, channels, height, width) to (batch_size, 1, input_size)\n",
    "            imgs = imgs.view(imgs.size(0), 1, -1)\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    return all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a test_loader similar to train_loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test the model\n",
    "all_preds, all_labels = test_model(test_loader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=subfolders))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=subfolders, yticklabels=subfolders)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to test the model and display accuracy\n",
    "def test_model(test_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Load the best saved model\n",
    "model = LSTMImageClassifier(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Assuming you have a test_loader defined\n",
    "test_model(test_loader, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
